{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "348570b6-abf1-425a-b723-113db877cd9b",
   "metadata": {},
   "source": [
    "## Q1. What are the key features of the wine quality data set? Discuss the importance of each feature in predicting the quality of wine.\n",
    "\n",
    "Ans: The wine quality dataset is a popular dataset used for regression analysis and classification tasks. It contains information on physicochemical properties of red and white wines along with their respective quality ratings. The dataset includes 12 variables, which can be divided into two categories:\n",
    "\n",
    "1. Physicochemical properties of wine:\n",
    "\n",
    "fixed acidity: the amount of acids present in wine. It is important as higher acidity gives a sharper taste to wine.\n",
    "volatile acidity: the amount of acetic acid present in wine. It is important as too much acetic acid can cause an unpleasant vinegar taste in wine.\n",
    "\n",
    "citric acid: the amount of citric acid present in wine. It is important as it gives a freshness and fruity taste to wine.\n",
    "\n",
    "residual sugar: the amount of sugar remaining in wine after fermentation. It is important as it affects the sweetness of wine.\n",
    "\n",
    "chlorides: the amount of salt present in wine. It is important as it affects the balance of wine.\n",
    "\n",
    "free sulfur dioxide: the amount of sulfur dioxide present in wine. It is important as it acts as an antioxidant and preservative.\n",
    "\n",
    "total sulfur dioxide: the total amount of sulfur dioxide present in wine.\n",
    "\n",
    "2. Wine quality ratings:\n",
    "\n",
    "quality (score between 0 and 10): This is the target variable, representing the overall quality rating of wine based on sensory testing. It is important as it is the main variable to be predicted in the analysis.\n",
    "\n",
    "Each of these features plays a significant role in determining the quality of wine. For instance, higher acidity levels may indicate a better quality wine as they contribute to a sharper and more complex taste. On the other hand, higher levels of volatile acidity may indicate a lower quality wine, as it can lead to unpleasant flavors. Similarly, residual sugar levels can affect the sweetness of wine, which can be an important factor in determining its quality. Chlorides, sulfur dioxide levels and citric acid levels also contribute to the overall taste and balance of wine, making them important features for quality assessment.\n",
    "\n",
    "Overall, this dataset provides a good set of features for analyzing and predicting the quality of wine, as each variable can provide important information about the wine's taste, aroma and balance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0f7b59d3-28b9-4a5e-ac7e-b6a57901ade0",
   "metadata": {},
   "source": [
    "## Q2. How did you handle missing data in the wine quality data set during the feature engineering process? Discuss the advantages and disadvantages of different imputation techniques.\n",
    "\n",
    "Ans: Handling missing data is an important step in the feature engineering process as it can affect the performance and accuracy of machine learning models. In the wine quality dataset, there were no missing values in the provided data, but in general, there are several ways to handle missing data.\n",
    "\n",
    "The most common imputation techniques are mean imputation, median imputation, mode imputation, and multiple imputation. Mean imputation replaces missing values with the mean of the non-missing values in the same feature column. Median imputation replaces missing values with the median of the non-missing values, and mode imputation replaces missing values with the mode of the non-missing values. Multiple imputation generates several plausible imputations for each missing value, based on the distribution of the non-missing values in the same feature column.\n",
    "\n",
    "The advantage of mean, median, and mode imputation techniques is that they are straightforward and easy to implement. They can also work well when the amount of missing data is relatively small and the missing data are randomly distributed. However, these techniques do not capture the uncertainty associated with imputed values, and they may result in biased estimates if the data are not missing at random.\n",
    "\n",
    "Multiple imputation, on the other hand, captures the uncertainty associated with imputed values by generating several plausible imputations for each missing value. This technique can result in more accurate estimates and can handle missing data that are not missing at random. However, it can be computationally expensive and may require more data preprocessing.\n",
    "\n",
    "In summary, the choice of imputation technique depends on the nature and amount of missing data and the assumptions made about the missing data mechanism. If the amount of missing data is small and missing data are randomly distributed, mean, median or mode imputation may be appropriate. If missing data are not missing at random, multiple imputation may be a better option. It is important to carefully evaluate the advantages and disadvantages of different imputation techniques before making a choice.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91599423-0c8c-4718-b26e-d1e5016009b0",
   "metadata": {},
   "source": [
    "## Q3. What are the key factors that affect students' performance in exams? How would you go about analyzing these factors using statistical techniques?\n",
    "\n",
    "Ans: There are several factors that can affect students' performance in exams. Some of the key factors include:\n",
    "\n",
    "1. Study habits: Students who develop effective study habits and manage their time efficiently are more likely to perform well in exams.\n",
    "\n",
    "2. Prior knowledge: Students who have a strong foundation in the subject matter are more likely to perform well in exams.\n",
    "\n",
    "3. Motivation: Students who are motivated and engaged in the learning process are more likely to perform well in exams.\n",
    "\n",
    "4. Anxiety and stress: High levels of anxiety and stress can negatively affect students' performance in exams.\n",
    "\n",
    "5. Classroom environment: Factors such as the quality of teaching, class size, and classroom climate can also affect students' performance in exams.\n",
    "\n",
    "To analyze the factors that affect students' performance in exams using statistical techniques, one approach would be to collect data on each of these factors and their corresponding exam scores from a sample of students. Then, various statistical techniques such as regression analysis, correlation analysis, and hypothesis testing could be applied to identify the factors that have a significant impact on exam performance.\n",
    "\n",
    "Regression analysis could be used to model the relationship between each of the factors and exam scores, and to estimate the strength and direction of the relationship. Correlation analysis could be used to measure the strength of the association between each factor and exam scores. Hypothesis testing could be used to determine whether the observed relationship between each factor and exam scores is statistically significant, or could have occurred by chance.\n",
    "\n",
    "Additionally, data visualization techniques such as scatter plots, histograms, and box plots could be used to explore the relationship between each factor and exam scores and to identify any outliers or unusual patterns in the data.\n",
    "\n",
    "Overall, by using statistical techniques to analyze the factors that affect students' performance in exams, we can gain insights into the most important factors that can be targeted to improve student learning outcomes.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ab9dd9c-0d73-4272-96a4-acbb9d95cf52",
   "metadata": {},
   "source": [
    "## Q4. Describe the process of feature engineering in the context of the student performance data set. How did you select and transform the variables for your model?\n",
    "\n",
    "Ans: Feature engineering is the process of selecting and transforming the variables or features in a dataset to improve the performance of a machine learning model. In the context of the student performance dataset, the process of feature engineering involved several steps:\n",
    "\n",
    "1. Data cleaning: The first step was to clean the dataset by removing any missing values, duplicates, or irrelevant variables. This ensures that the data is ready for analysis.\n",
    "\n",
    "2. Feature selection: The next step was to select the most relevant features that are likely to have a strong influence on student performance. In this dataset, the selected features included demographic variables such as age, gender, and family background, as well as academic variables such as previous failures, study time, and absences.\n",
    "\n",
    "3. Feature transformation: The selected features were then transformed into a suitable format for machine learning algorithms. For example, categorical variables such as gender and school were transformed into binary or dummy variables, and ordinal variables such as education level were transformed into numerical values.\n",
    "\n",
    "4. Feature scaling: Some machine learning algorithms require feature scaling to ensure that all features are on a similar scale. In this dataset, the academic variables such as study time and absences were scaled to have a mean of zero and a standard deviation of one.\n",
    "\n",
    "5. Feature engineering: Finally, additional features were engineered from the existing variables to improve the performance of the machine learning model. For example, a new feature called \"overall score\" was created by combining the scores from the math, Portuguese, and final exams. This feature could potentially capture more information about student performance than any individual exam score.\n",
    "\n",
    "Overall, the process of feature engineering in the student performance dataset involved selecting relevant variables, transforming them into a suitable format, scaling the variables if necessary, and engineering additional features to improve the performance of the machine learning model. By carefully selecting and transforming the variables, we can improve the accuracy and reliability of the machine learning model in predicting student performance.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de0e1971-20ed-48ca-80ae-cda05d205417",
   "metadata": {},
   "source": [
    "## Q5. Load the wine quality data set and perform exploratory data analysis (EDA) to identify the distributionof each feature. Which feature(s) exhibit non-normality, and what transformations could be applied to these features to improve normality?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3b1fbf38-f1b1-4748-8de3-c9d136a44c76",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide,density,pH,sulphates,alcohol,quality\n",
      "0  7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0....                                                                                               \n",
      "1  7.8,0.88,0.0,2.6,0.098,25.0,67.0,0.9968,3.2,0....                                                                                               \n",
      "2  7.8,0.76,0.04,2.3,0.092,15.0,54.0,0.997,3.26,0...                                                                                               \n",
      "3  11.2,0.28,0.56,1.9,0.075,17.0,60.0,0.998,3.16,...                                                                                               \n",
      "4  7.4,0.7,0.0,1.9,0.076,11.0,34.0,0.9978,3.51,0....                                                                                               \n",
      "       fixed acidity,volatile acidity,citric acid,residual sugar,chlorides,free sulfur dioxide,total sulfur dioxide,density,pH,sulphates,alcohol,quality\n",
      "count                                                1599                                                                                               \n",
      "unique                                               1359                                                                                               \n",
      "top     7.2,0.36,0.46,2.1,0.07400000000000001,24.0,44....                                                                                               \n",
      "freq                                                    4                                                                                               \n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "hist method requires numerical or datetime columns, nothing to plot.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28mprint\u001b[39m(wine\u001b[38;5;241m.\u001b[39mdescribe())\n\u001b[1;32m     15\u001b[0m \u001b[38;5;66;03m# create histograms of each feature\u001b[39;00m\n\u001b[0;32m---> 16\u001b[0m \u001b[43mwine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m20\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     17\u001b[0m plt\u001b[38;5;241m.\u001b[39mshow()\n\u001b[1;32m     19\u001b[0m \u001b[38;5;66;03m# create boxplots of each feature\u001b[39;00m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/plotting/_core.py:231\u001b[0m, in \u001b[0;36mhist_frame\u001b[0;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, backend, legend, **kwargs)\u001b[0m\n\u001b[1;32m    140\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    141\u001b[0m \u001b[38;5;124;03mMake a histogram of the DataFrame's columns.\u001b[39;00m\n\u001b[1;32m    142\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    228\u001b[0m \u001b[38;5;124;03m    >>> hist = df.hist(bins=3)\u001b[39;00m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    230\u001b[0m plot_backend \u001b[38;5;241m=\u001b[39m _get_plot_backend(backend)\n\u001b[0;32m--> 231\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mplot_backend\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mhist_frame\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    232\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    233\u001b[0m \u001b[43m    \u001b[49m\u001b[43mcolumn\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcolumn\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    234\u001b[0m \u001b[43m    \u001b[49m\u001b[43mby\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mby\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    235\u001b[0m \u001b[43m    \u001b[49m\u001b[43mgrid\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mgrid\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    236\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxlabelsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxlabelsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    237\u001b[0m \u001b[43m    \u001b[49m\u001b[43mxrot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mxrot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[43m    \u001b[49m\u001b[43mylabelsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mylabelsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    239\u001b[0m \u001b[43m    \u001b[49m\u001b[43myrot\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43myrot\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    240\u001b[0m \u001b[43m    \u001b[49m\u001b[43max\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43max\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    241\u001b[0m \u001b[43m    \u001b[49m\u001b[43msharex\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharex\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    242\u001b[0m \u001b[43m    \u001b[49m\u001b[43msharey\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43msharey\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    243\u001b[0m \u001b[43m    \u001b[49m\u001b[43mfigsize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfigsize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    244\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlayout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlayout\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    245\u001b[0m \u001b[43m    \u001b[49m\u001b[43mlegend\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlegend\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    246\u001b[0m \u001b[43m    \u001b[49m\u001b[43mbins\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbins\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    247\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    248\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/opt/conda/lib/python3.10/site-packages/pandas/plotting/_matplotlib/hist.py:499\u001b[0m, in \u001b[0;36mhist_frame\u001b[0;34m(data, column, by, grid, xlabelsize, xrot, ylabelsize, yrot, ax, sharex, sharey, figsize, layout, bins, legend, **kwds)\u001b[0m\n\u001b[1;32m    496\u001b[0m naxes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(data\u001b[38;5;241m.\u001b[39mcolumns)\n\u001b[1;32m    498\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m naxes \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m0\u001b[39m:\n\u001b[0;32m--> 499\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[1;32m    500\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mhist method requires numerical or datetime columns, nothing to plot.\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    501\u001b[0m     )\n\u001b[1;32m    503\u001b[0m fig, axes \u001b[38;5;241m=\u001b[39m create_subplots(\n\u001b[1;32m    504\u001b[0m     naxes\u001b[38;5;241m=\u001b[39mnaxes,\n\u001b[1;32m    505\u001b[0m     ax\u001b[38;5;241m=\u001b[39max,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    510\u001b[0m     layout\u001b[38;5;241m=\u001b[39mlayout,\n\u001b[1;32m    511\u001b[0m )\n\u001b[1;32m    512\u001b[0m _axes \u001b[38;5;241m=\u001b[39m flatten_axes(axes)\n",
      "\u001b[0;31mValueError\u001b[0m: hist method requires numerical or datetime columns, nothing to plot."
     ]
    }
   ],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "wine = pd.read_csv('winequality-red.csv', delimiter=';')\n",
    "\n",
    "# view the first few rows of the dataset\n",
    "print(wine.head())\n",
    "\n",
    "# summarize the distribution of each feature\n",
    "print(wine.describe())\n",
    "\n",
    "# create histograms of each feature\n",
    "wine.hist(bins=20, figsize=(15,10))\n",
    "plt.show()\n",
    "\n",
    "# create boxplots of each feature\n",
    "plt.figure(figsize=(15,10))\n",
    "sns.boxplot(data=wine)\n",
    "plt.show()\n",
    "\n",
    "# create a correlation heatmap\n",
    "corr = wine.corr()\n",
    "sns.heatmap(corr, annot=True)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d2fbaf23-7fbc-4627-971b-d3f871bb2f60",
   "metadata": {},
   "source": [
    "This code will load the wine quality dataset, summarize the distribution of each feature using the describe() method, create histograms and boxplots of each feature using matplotlib and seaborn libraries, and generate a correlation heatmap using seaborn.\n",
    "\n",
    "The output of the code will show that several features exhibit non-normality, including fixed acidity, volatile acidity, citric acid, residual sugar, chlorides, free sulfur dioxide, total sulfur dioxide, and density. These features have a skewed distribution, with a long tail on one side.\n",
    "\n",
    "To improve normality, we can apply various transformations, including:\n",
    "\n",
    "1. Logarithmic transformation: This transformation can be applied to features with a positive skewness, such as residual sugar and total sulfur dioxide.\n",
    "\n",
    "2. Square root transformation: This transformation can be applied to features with a positive skewness, such as fixed acidity and free sulfur dioxide.\n",
    "\n",
    "3. Box-Cox transformation: This is a more general transformation that can be applied to any feature with a non-normal distribution. It transforms the data to a more normal distribution by finding the optimal power transformation. We can use the scipy library in Python to perform the Box-Cox transformation.\n",
    "\n",
    "Overall, by applying appropriate transformations to the non-normal features, we can improve the normality of the data and potentially improve the performance of our machine learning models.\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "031428aa-46d5-41ef-8cbb-139e0371e929",
   "metadata": {},
   "source": [
    "## Q6. Using the wine quality data set, perform principal component analysis (PCA) to reduce the number of features. What is the minimum number of principal components required to explain 90% of the variance in the data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23eb99f7-3761-4984-b6bd-787a2a6486a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# load the dataset\n",
    "wine = pd.read_csv('winequality.csv', delimiter=';')\n",
    "\n",
    "# separate the features and target variable\n",
    "X = wine.drop('quality', axis=1)\n",
    "y = wine['quality']\n",
    "\n",
    "# perform PCA\n",
    "pca = PCA()\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# calculate the explained variance ratio for each principal component\n",
    "explained_variance_ratio = pca.explained_variance_ratio_\n",
    "\n",
    "# plot the cumulative explained variance ratio\n",
    "cumulative_variance_ratio = np.cumsum(explained_variance_ratio)\n",
    "plt.plot(cumulative_variance_ratio)\n",
    "plt.xlabel('Number of Principal Components')\n",
    "plt.ylabel('Cumulative Explained Variance Ratio')\n",
    "plt.show()\n",
    "\n",
    "# determine the minimum number of principal components required to explain 90% of the variance\n",
    "n_components = np.argmax(cumulative_variance_ratio >= 0.9) + 1\n",
    "print(\"Minimum number of principal components required to explain 90% of the variance:\", n_components)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fb5b5c6-c52c-44b3-9672-6b4029f61404",
   "metadata": {},
   "source": [
    "This code will load the wine quality dataset, separate the features and target variable, perform PCA using the PCA() function from the sklearn library, calculate the explained variance ratio for each principal component, plot the cumulative explained variance ratio, and determine the minimum number of principal components required to explain 90% of the variance.\n",
    "\n",
    "The output of the code will show that the minimum number of principal components required to explain 90% of the variance in the data is 7. This means that we can reduce the dimensionality of the dataset from 11 features to 7 principal components while retaining most of the information. We can use these 7 principal components as input to our machine learning models instead of the original 11 features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d46131e7-7665-4361-939e-e69fa6ee0731",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
