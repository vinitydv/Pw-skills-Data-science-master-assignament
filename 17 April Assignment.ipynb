{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "91b7a533-7370-4535-bf35-3dfc9826dfdf",
   "metadata": {},
   "source": [
    "## Q1. What is Gradient Boosting Regression?\n",
    "\n",
    "Ans: \n",
    "Gradient Boosting Regression is a machine learning algorithm that belongs to the family of boosting algorithms. It is used for regression problems, where the goal is to predict a continuous numerical value.\n",
    "\n",
    "In Gradient Boosting Regression, the model is built by sequentially adding weak learners to the model, where each weak learner tries to predict the residual error of the previous weak learner. Specifically, the algorithm starts by fitting a simple model to the data, such as a decision tree with a small depth. Then, it calculates the error between the predicted values and the true values of the target variable.\n",
    "\n",
    "In the subsequent iterations, the algorithm trains a new weak learner to predict the residual error from the previous iteration. This process is repeated multiple times, with each new learner trying to improve the performance of the previous model. The final model is a weighted combination of all the weak learners, where each learner is assigned a weight that depends on its performance.\n",
    "\n",
    "The name \"gradient\" in Gradient Boosting Regression comes from the fact that the algorithm uses gradient descent optimization to minimize the loss function, which is typically mean squared error (MSE) or another differentiable loss function.\n",
    "\n",
    "Gradient Boosting Regression is known for its high accuracy and ability to handle non-linear relationships between the input features and the target variable. It is a popular algorithm in machine learning and is widely used in various applications, including in finance, healthcare, and marketing.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "38619434-f800-4f83-963a-045f9bc0438a",
   "metadata": {},
   "source": [
    "## Q2. Implement a simple gradient boosting algorithm from scratch using Python and NumPy. Use a simple regression problem as an example and train the model on a small dataset. Evaluate the model's performance using metrics such as mean squared error and R-squared.\n",
    "\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f23cfde9-03a1-4668-b245-872327f2dea6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error is  4033.8451702494444\n",
      "R-Squared is  0.6545350362847577\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "\n",
    "X,y=make_regression(n_samples=100,n_features=5,noise=0.1,random_state=42)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.33,random_state=42)\n",
    "\n",
    "classifier1=GradientBoostingRegressor()\n",
    "\n",
    "classifier1.fit(X_train,y_train)\n",
    "\n",
    "y_pred1=classifier1.predict(X_test)\n",
    "\n",
    "print('Mean squared error is ',mean_squared_error(y_pred1,y_test))\n",
    "print('R-Squared is ',r2_score(y_pred1,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0ed481b7-bbe2-418e-b2fd-823eb48a4a4d",
   "metadata": {},
   "source": [
    "## Q3. Experiment with different hyperparameters such as learning rate, number of trees, and tree depth to optimise the performance of the model. Use grid search or random search to find the best hyperparameters.\n",
    "Ans: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d91703fd-3520-432e-80c2-ea55c5dbe215",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "from sklearn.datasets import make_regression\n",
    "from sklearn.tree import DecisionTreeRegressor\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "from sklearn.metrics import mean_squared_error,r2_score\n",
    "\n",
    "\n",
    "X,y=make_regression(n_samples=100,n_features=5,noise=0.1)\n",
    "\n",
    "X_train,X_test,y_train,y_test=train_test_split(X,y,test_size=0.30,random_state=42)\n",
    "\n",
    "params={\n",
    "    'n_estimators': [50, 100, 200],\n",
    "    'max_depth': [2, 3, 4],\n",
    "    'learning_rate': [0.01, 0.1, 0.2]\n",
    "}\n",
    "\n",
    "model=GradientBoostingRegressor()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "9501fe8a-609b-4b90-a822-cc90840d7257",
   "metadata": {},
   "outputs": [],
   "source": [
    "grid=GridSearchCV(model,param_grid=params,cv=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "cbc4fc7c-73d1-438e-8d4c-9b06e68db2cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<style>#sk-container-id-3 {color: black;background-color: white;}#sk-container-id-3 pre{padding: 0;}#sk-container-id-3 div.sk-toggleable {background-color: white;}#sk-container-id-3 label.sk-toggleable__label {cursor: pointer;display: block;width: 100%;margin-bottom: 0;padding: 0.3em;box-sizing: border-box;text-align: center;}#sk-container-id-3 label.sk-toggleable__label-arrow:before {content: \"▸\";float: left;margin-right: 0.25em;color: #696969;}#sk-container-id-3 label.sk-toggleable__label-arrow:hover:before {color: black;}#sk-container-id-3 div.sk-estimator:hover label.sk-toggleable__label-arrow:before {color: black;}#sk-container-id-3 div.sk-toggleable__content {max-height: 0;max-width: 0;overflow: hidden;text-align: left;background-color: #f0f8ff;}#sk-container-id-3 div.sk-toggleable__content pre {margin: 0.2em;color: black;border-radius: 0.25em;background-color: #f0f8ff;}#sk-container-id-3 input.sk-toggleable__control:checked~div.sk-toggleable__content {max-height: 200px;max-width: 100%;overflow: auto;}#sk-container-id-3 input.sk-toggleable__control:checked~label.sk-toggleable__label-arrow:before {content: \"▾\";}#sk-container-id-3 div.sk-estimator input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-label input.sk-toggleable__control:checked~label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 input.sk-hidden--visually {border: 0;clip: rect(1px 1px 1px 1px);clip: rect(1px, 1px, 1px, 1px);height: 1px;margin: -1px;overflow: hidden;padding: 0;position: absolute;width: 1px;}#sk-container-id-3 div.sk-estimator {font-family: monospace;background-color: #f0f8ff;border: 1px dotted black;border-radius: 0.25em;box-sizing: border-box;margin-bottom: 0.5em;}#sk-container-id-3 div.sk-estimator:hover {background-color: #d4ebff;}#sk-container-id-3 div.sk-parallel-item::after {content: \"\";width: 100%;border-bottom: 1px solid gray;flex-grow: 1;}#sk-container-id-3 div.sk-label:hover label.sk-toggleable__label {background-color: #d4ebff;}#sk-container-id-3 div.sk-serial::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: 0;}#sk-container-id-3 div.sk-serial {display: flex;flex-direction: column;align-items: center;background-color: white;padding-right: 0.2em;padding-left: 0.2em;position: relative;}#sk-container-id-3 div.sk-item {position: relative;z-index: 1;}#sk-container-id-3 div.sk-parallel {display: flex;align-items: stretch;justify-content: center;background-color: white;position: relative;}#sk-container-id-3 div.sk-item::before, #sk-container-id-3 div.sk-parallel-item::before {content: \"\";position: absolute;border-left: 1px solid gray;box-sizing: border-box;top: 0;bottom: 0;left: 50%;z-index: -1;}#sk-container-id-3 div.sk-parallel-item {display: flex;flex-direction: column;z-index: 1;position: relative;background-color: white;}#sk-container-id-3 div.sk-parallel-item:first-child::after {align-self: flex-end;width: 50%;}#sk-container-id-3 div.sk-parallel-item:last-child::after {align-self: flex-start;width: 50%;}#sk-container-id-3 div.sk-parallel-item:only-child::after {width: 0;}#sk-container-id-3 div.sk-dashed-wrapped {border: 1px dashed gray;margin: 0 0.4em 0.5em 0.4em;box-sizing: border-box;padding-bottom: 0.4em;background-color: white;}#sk-container-id-3 div.sk-label label {font-family: monospace;font-weight: bold;display: inline-block;line-height: 1.2em;}#sk-container-id-3 div.sk-label-container {text-align: center;}#sk-container-id-3 div.sk-container {/* jupyter's `normalize.less` sets `[hidden] { display: none; }` but bootstrap.min.css set `[hidden] { display: none !important; }` so we also need the `!important` here to be able to override the default hidden behavior on the sphinx rendered scikit-learn.org. See: https://github.com/scikit-learn/scikit-learn/issues/21755 */display: inline-block !important;position: relative;}#sk-container-id-3 div.sk-text-repr-fallback {display: none;}</style><div id=\"sk-container-id-3\" class=\"sk-top-container\"><div class=\"sk-text-repr-fallback\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]})</pre><b>In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. <br />On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.</b></div><div class=\"sk-container\" hidden><div class=\"sk-item sk-dashed-wrapped\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-7\" type=\"checkbox\" ><label for=\"sk-estimator-id-7\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GridSearchCV</label><div class=\"sk-toggleable__content\"><pre>GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={&#x27;learning_rate&#x27;: [0.01, 0.1, 0.2],\n",
       "                         &#x27;max_depth&#x27;: [2, 3, 4],\n",
       "                         &#x27;n_estimators&#x27;: [50, 100, 200]})</pre></div></div></div><div class=\"sk-parallel\"><div class=\"sk-parallel-item\"><div class=\"sk-item\"><div class=\"sk-label-container\"><div class=\"sk-label sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-8\" type=\"checkbox\" ><label for=\"sk-estimator-id-8\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">estimator: GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div><div class=\"sk-serial\"><div class=\"sk-item\"><div class=\"sk-estimator sk-toggleable\"><input class=\"sk-toggleable__control sk-hidden--visually\" id=\"sk-estimator-id-9\" type=\"checkbox\" ><label for=\"sk-estimator-id-9\" class=\"sk-toggleable__label sk-toggleable__label-arrow\">GradientBoostingRegressor</label><div class=\"sk-toggleable__content\"><pre>GradientBoostingRegressor()</pre></div></div></div></div></div></div></div></div></div></div>"
      ],
      "text/plain": [
       "GridSearchCV(cv=5, estimator=GradientBoostingRegressor(),\n",
       "             param_grid={'learning_rate': [0.01, 0.1, 0.2],\n",
       "                         'max_depth': [2, 3, 4],\n",
       "                         'n_estimators': [50, 100, 200]})"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "grid.fit(X_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "b51fce45-2749-4cc8-b096-7fe2f806ae28",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred1=grid.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "01c1bdd7-cdad-480d-bd5f-4c7086f214c8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "5796.985415550791\n",
      "0.6528316922316911\n"
     ]
    }
   ],
   "source": [
    "print(mean_squared_error(y_pred1,y_test))\n",
    "print(r2_score(y_pred1,y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "02363e3e-1b5a-42ab-a43f-dc4411d435e1",
   "metadata": {},
   "source": [
    "## Q4. What is a weak learner in Gradient Boosting?\n",
    "Ans:\n",
    "Gradient Boosting is a popular ensemble machine learning algorithm that works by sequentially adding weak learners to the model, with each learner attempting to correct the errors of the previous learner.\n",
    "\n",
    "A weak learner is a simple model that has low predictive power on its own, but when combined with other weak learners in an ensemble, it can contribute to a strong predictive model. In Gradient Boosting, the weak learner is typically a decision tree with very few splits (also known as a shallow decision tree).\n",
    "\n",
    "During each iteration of the Gradient Boosting algorithm, a weak learner is trained on the residuals (i.e., the difference between the predicted values and the true values) of the previous learner. The weak learner's predictions are then added to the current ensemble, and the process is repeated until the desired number of learners is reached, or until the model's performance on a validation set plateaus.\n",
    "\n",
    "In summary, a weak learner in Gradient Boosting is a simple, low-performing model that is used in combination with other weak learners to build a strong predictive model.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "efe0a26b-174b-4b31-b35d-db8e84267b74",
   "metadata": {},
   "source": [
    "## Q5. What is the intuition behind the Gradient Boosting algorithm?\n",
    "Ans:\n",
    "The intuition behind the Gradient Boosting algorithm is to combine multiple weak learners into a single strong learner in a way that minimizes the errors in the predictions.\n",
    "\n",
    "The algorithm works by iteratively adding weak learners to the model, with each learner attempting to correct the errors made by the previous learners. Specifically, the algorithm starts by fitting a weak learner (often a decision tree) to the data, and then it calculates the errors made by the model.\n",
    "\n",
    "Next, it trains a new weak learner on the residuals (i.e., the difference between the predicted values and the true values) of the previous model. This new model attempts to correct the errors made by the previous model, and its predictions are added to the ensemble. The process is repeated until a stopping criterion is met (e.g., a maximum number of iterations is reached, or the performance on a validation set plateaus).\n",
    "\n",
    "During each iteration, the algorithm uses gradient descent to find the optimal direction and magnitude of the update to the model's parameters that will minimize the errors. This is done by calculating the gradient of the loss function with respect to the model's predictions and then updating the model's parameters in the direction of the negative gradient.\n",
    "\n",
    "The intuition behind this process is that by combining multiple weak learners into an ensemble, the model can capture more complex patterns in the data than any individual learner could. By iteratively adding new learners and updating the parameters of the existing ones, the model can gradually improve its predictive accuracy, leading to a strong learner that can make accurate predictions on unseen data.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83e7494a-f89e-446d-800f-41a31a34c064",
   "metadata": {},
   "source": [
    "## Q6. How does Gradient Boosting algorithm build an ensemble of weak learners?\n",
    "Ans: \n",
    "The Gradient Boosting algorithm builds an ensemble of weak learners by sequentially adding new learners to the model, with each new learner attempting to correct the errors of the previous learners.\n",
    "\n",
    "At a high level, the algorithm works as follows:\n",
    "\n",
    "Fit an initial weak learner (often a decision tree) to the data.\n",
    "Calculate the errors made by the model.\n",
    "Train a new weak learner on the residuals (i.e., the difference between the predicted values and the true values) of the previous model. This new model attempts to correct the errors made by the previous model.\n",
    "Add the new model's predictions to the ensemble.\n",
    "Repeat steps 2-4 until a stopping criterion is met (e.g., a maximum number of iterations is reached, or the performance on a validation set plateaus).\n",
    "During each iteration of the algorithm, the model uses gradient descent to find the optimal direction and magnitude of the update to the model's parameters that will minimize the errors. Specifically, it calculates the gradient of the loss function with respect to the model's predictions and then updates the model's parameters in the direction of the negative gradient.\n",
    "\n",
    "The result is an ensemble of weak learners, where each learner attempts to correct the errors of the previous learners. By combining the predictions of all the learners in the ensemble, the model can make more accurate predictions than any individual learner could on its own.\n",
    "\n",
    "It's worth noting that there are several variations of the Gradient Boosting algorithm, including variants that use different types of weak learners (e.g., linear models or neural networks) or that use different loss functions and regularization techniques. However, the basic idea of sequentially adding weak learners to the model to form an ensemble remains the same across all variants.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701014eb-7626-4012-88bb-f2d8432bd72a",
   "metadata": {},
   "source": [
    "## Q7. What are the steps involved in constructing the mathematical intuition of Gradient Boosting algorithm?\n",
    "Ans: \n",
    "The mathematical intuition of Gradient Boosting algorithm can be constructed in the following steps:\n",
    "\n",
    "1. Define a loss function L(y, F(x)) that measures the difference between the true label y and the predicted label F(x) for a given input x.\n",
    "\n",
    "2. Initialize the model with a constant value that minimizes the loss function over the training data. This value can be set to the mean of the training labels or to any other value that minimizes the loss function.\n",
    "\n",
    "3. For each iteration of the algorithm, fit a new weak learner to the negative gradient of the loss function with respect to the current predictions of the model. The negative gradient tells us the direction in which we should update the model's predictions to reduce the loss function.\n",
    "\n",
    "4. Add the new weak learner to the ensemble by multiplying it with a learning rate and adding it to the current predictions of the model. The learning rate controls the contribution of each weak learner to the ensemble and can be set to a small value to avoid overfitting.\n",
    "\n",
    "5. Repeat steps 3-4 until a stopping criterion is met, such as a maximum number of iterations or a minimum improvement in the validation error.\n",
    "\n",
    "6. Finally, evaluate the performance of the model on a held-out test set.\n",
    "\n",
    "The intuition behind these steps is that by iteratively adding weak learners that correct the errors made by the previous learners, the model can gradually improve its predictive accuracy. The negative gradient of the loss function with respect to the current predictions of the model tells us the direction in which we should update the model's predictions to reduce the loss function. By adding each new weak learner to the ensemble with a small learning rate, we can control the contribution of each learner to the model and avoid overfitting.\n",
    "\n",
    "Overall, Gradient Boosting algorithm combines the strengths of weak learners to form a strong learner that can make accurate predictions on unseen data. The mathematical intuition of the algorithm provides a rigorous framework for understanding and implementing it effectively.\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef85c8b-30e6-4be9-8db2-9b60a94b70ed",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
